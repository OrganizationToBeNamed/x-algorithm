# Real Example Scenario: A "For You" Feed Request

> This walks through exactly what happens when a real user opens their X app and sees the "For You" tab. Every stage is traced with concrete data.

**Navigation:** [Home](index.md) Â· [Architecture](ARCHITECTURE.md) Â· **Example Scenario** Â· [Pipeline Diagram](diagrams/pipeline-component-flow.md) Â· [Sequence Diagram](diagrams/sequence-diagram.md)

---

## Meet Our User: Priya (@priya_codes)

```
User ID:        900100200
Follows:        312 accounts (mix of tech, sports, friends)
Blocked:        2 accounts (IDs: 555000111, 555000222)
Muted:          1 account  (ID: 555000333)
Muted keywords: ["crypto giveaway", "drop your wallet"]
Country:        US
Language:       en
```

Priya opens the X app on her phone at **10:15am on a Monday**. The app makes a gRPC call:

```protobuf
GetScoredPosts {
    viewer_id:       900100200,
    client_app_id:   12345,
    country_code:    "US",
    language_code:   "en",
    seen_ids:        [1893000001, 1893000002, ...47 more...],   // 49 posts she already saw
    served_ids:      [1893000001, 1893000002, ...24 more...],   // 26 posts served in last session
    in_network_only: false,
    is_bottom_request: false,
    bloom_filter_entries: [<compressed bloom filter of ~500 recently seen IDs>],
}
```

---

## Stage 1: Query Hydration

Two hydrators run **in parallel**:

### UserActionSeqQueryHydrator
Fetches Priya's recent engagement history from the UAS service:

```
Priya's last 128 actions (most recent first):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Action          â”‚ Tweet ID     â”‚ Author ID   â”‚ Surface
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Liked (fav)     â”‚ 1893000050   â”‚ 300400500   â”‚ home_timeline
  Replied         â”‚ 1893000048   â”‚ 200300400   â”‚ home_timeline
  Retweeted       â”‚ 1893000045   â”‚ 300400500   â”‚ home_timeline
  Clicked         â”‚ 1893000040   â”‚ 100200300   â”‚ home_timeline
  Dwelled 15s     â”‚ 1893000038   â”‚ 400500600   â”‚ home_timeline
  Liked (fav)     â”‚ 1893000035   â”‚ 100200300   â”‚ search
  Video viewed    â”‚ 1893000030   â”‚ 500600700   â”‚ home_timeline
  ... (121 more actions going back ~3 days) ...
```

This engagement history becomes the **input sequence** for the Phoenix transformer â€” it's how the model "knows" what Priya likes.

### UserFeaturesQueryHydrator
Fetches from Strato:

```
UserFeatures {
    followed_user_ids:    [100200300, 200300400, 300400500, ... 309 more],
    blocked_user_ids:     [555000111, 555000222],
    muted_user_ids:       [555000333],
    muted_keywords:       ["crypto giveaway", "drop your wallet"],
    subscribed_user_ids:  [100200300],
}
```

**Result:** The `ScoredPostsQuery` is now enriched with Priya's full context.

**Time elapsed: ~12ms** (both hydrators ran in parallel)

---

## Stage 2: Candidate Sources

Two sources run **in parallel**:

### ThunderSource (In-Network)

Calls Thunder gRPC with Priya's 312 followed user IDs:

```
GetInNetworkPosts {
    user_id:            900100200,
    following_user_ids: [100200300, 200300400, 300400500, ...],
    max_results:        500,
    algorithm:          "default",
}
```

Thunder looks up its in-memory PostStore â€” for each of the 312 followed accounts, it retrieves recent original posts and some replies/retweets. Returns **487 candidate posts**.

Example candidates from Thunder:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  tweet_id       â”‚ author_id   â”‚ @handle          â”‚ type     â”‚ age
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1893100001     â”‚ 100200300   â”‚ @techguru        â”‚ original â”‚ 2h
  1893100002     â”‚ 100200300   â”‚ @techguru        â”‚ reply    â”‚ 1h
  1893100003     â”‚ 200300400   â”‚ @sportsfan       â”‚ original â”‚ 4h
  1893100004     â”‚ 200300400   â”‚ @sportsfan       â”‚ retweet  â”‚ 3h
  1893100005     â”‚ 300400500   â”‚ @designeramy     â”‚ original â”‚ 30m
  1893100006     â”‚ 300400500   â”‚ @designeramy     â”‚ original â”‚ 6h
  1893100007     â”‚ 400500600   â”‚ @newsbot         â”‚ original â”‚ 1h
  1893100008     â”‚ 555000333   â”‚ @annoying_person â”‚ original â”‚ 2h   â† muted user!
  ... (479 more) ...
```

All arrive with `served_type: ForYouInNetwork`.

### PhoenixSource (Out-of-Network)

Calls Phoenix Retrieval with Priya's engagement history embedding:

```
Retrieve {
    user_id:              900100200,
    user_action_sequence: <128-action history>,
    max_results:          500,
}
```

Phoenix Retrieval internally:
1. Encodes Priya's history through the **User Tower** (Grok transformer) â†’ gets a 128-dim L2-normalized user embedding
2. Computes dot-product similarity against **millions of candidate embeddings** (pre-computed by Candidate Tower)
3. Returns top-500 by similarity score

Example candidates from Phoenix:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  tweet_id       â”‚ author_id   â”‚ type     â”‚ why retrieved
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1893200001     â”‚ 600700800   â”‚ original â”‚ Similar to tech posts Priya likes
  1893200002     â”‚ 700800900   â”‚ original â”‚ Similar to design content
  1893200003     â”‚ 800900100   â”‚ video    â”‚ Sports highlight (video, 45s)
  1893200004     â”‚ 900100200   â”‚ original â”‚ Priya's OWN post! (will be filtered)
  1893200005     â”‚ 555000111   â”‚ original â”‚ From blocked user! (will be filtered)
  1893200006     â”‚ 111222333   â”‚ original â”‚ Trending tech thread
  ... (494 more) ...
```

All arrive with `served_type: ForYouPhoenixRetrieval`.

**Combined total: 487 + 500 = 987 candidates**

**Time elapsed: ~45ms** (both sources ran in parallel; Thunder ~3ms, Phoenix ~42ms)

---

## Stage 3: Candidate Hydration

Five hydrators run **in parallel** to enrich all 987 candidates:

### InNetworkCandidateHydrator
Checks each candidate's `author_id` against Priya's followed list:
```
Thunder posts  â†’ in_network: true    (487 posts)
Phoenix posts  â†’ in_network: false   (500 posts, with exceptions if Phoenix 
                                       happened to retrieve someone Priya follows)
```

### CoreDataCandidateHydrator (via TES)
Batch-fetches core tweet data for all 987 IDs:
```
tweet_id: 1893100001 â†’ text: "Just shipped v2.0 of our Rust framework! ğŸ¦€"
tweet_id: 1893100005 â†’ text: "New design system drop â€” check the thread ğŸ§µ"
tweet_id: 1893200001 â†’ text: "Here's why async Rust changed our production infra..."
tweet_id: 1893100099 â†’ FAILED (tweet deleted during fetch) â†’ No core data
```
3 posts fail hydration (will be caught by `CoreDataHydrationFilter`).

### VideoDurationCandidateHydrator (via TES)
```
tweet_id: 1893200003 â†’ video_duration_ms: 45000  (45s sports highlight)
tweet_id: 1893100050 â†’ video_duration_ms: 8000   (8s clip)
tweet_id: 1893100077 â†’ video_duration_ms: 120000 (2min tutorial)
... (most posts have no video â†’ None)
```

### SubscriptionHydrator (via TES)
```
tweet_id: 1893100090 â†’ subscription_author_id: Some(444555666)
    (Paywalled post â€” Priya doesn't subscribe to 444555666)
tweet_id: 1893100001 â†’ subscription_author_id: None (free post)
```

### GizmoduckCandidateHydrator (via Gizmoduck)
Fetches author profiles for all unique author IDs:
```
author_id: 100200300 â†’ screen_name: "techguru",     followers: 45_200
author_id: 200300400 â†’ screen_name: "sportsfan",    followers: 12_800
author_id: 300400500 â†’ screen_name: "designeramy",  followers: 89_400
author_id: 600700800 â†’ screen_name: "rustacean_dev",followers: 156_000
author_id: 555000111 â†’ screen_name: "blocked_user", followers: 2_300
...
```

**Time elapsed: ~65ms** (all 5 hydrators run in parallel; TES calls dominate)

---

## Stage 4: Pre-Scoring Filters

10 filters run **sequentially**. Let's trace the candidate count through each:

```
Starting candidates: 987
```

### Filter 1: DropDuplicatesFilter
Phoenix and Thunder sometimes return the same tweet (e.g., if a followed user's post also ranks high in retrieval).
```
Removed: 12 duplicates
Remaining: 975
```

### Filter 2: CoreDataHydrationFilter
Removes posts where TES failed to return data:
```
Removed: 3 (deleted tweets, TES errors)
Remaining: 972
```

### Filter 3: AgeFilter
Removes posts older than the configured threshold (e.g., 48 hours):
```
Removed: 38 (old posts from less-active followed accounts)
Remaining: 934
```

### Filter 4: SelfTweetFilter
Checks `author_id != viewer_id (900100200)`:
```
Removed: 2 (Priya's own posts that Phoenix Retrieval found)
Remaining: 932
```

### Filter 5: RetweetDeduplicationFilter
If multiple people retweeted the same original tweet, keep only one:
```
Removed: 15 (e.g., tweet 1893050000 was retweeted by 3 people Priya follows)
Remaining: 917
```

### Filter 6: IneligibleSubscriptionFilter
Removes paywalled posts user can't access:
```
Removed: 4 (subscription content from non-subscribed authors)
Remaining: 913
```

### Filter 7: PreviouslySeenPostsFilter
Uses Priya's `seen_ids` + bloom filter to remove already-seen posts:
```
Removed: 31 (posts Priya scrolled past in earlier sessions)
Remaining: 882
```

### Filter 8: PreviouslyServedPostsFilter
Removes posts served in the current session:
```
Removed: 8 (posts from her last pull-to-refresh 20 min ago)
Remaining: 874
```

### Filter 9: MutedKeywordFilter
Tokenizes each post's text and matches against `["crypto giveaway", "drop your wallet"]`:
```
tweet_id: 1893200099 â€” text: "ğŸš€ CRYPTO GIVEAWAY! Drop your wallet address..."
  â†’ MATCH on "crypto giveaway" â€” REMOVED

Removed: 6 (spam/promo posts matching muted keywords)
Remaining: 868
```

### Filter 10: AuthorSocialgraphFilter
Checks `author_id` against blocked `[555000111, 555000222]` and muted `[555000333]`:
```
tweet_id: 1893200005 â€” author: 555000111 (blocked) â€” REMOVED
tweet_id: 1893100008 â€” author: 555000333 (muted)   â€” REMOVED

Removed: 5 (posts from blocked/muted authors)
Remaining: 863
```

**Summary of filtering:**
```
987 â†’ 863 candidates (124 removed, ~12.6% filtered out)
```

**Time elapsed: ~2ms** (all filters are in-memory operations, no network calls)

---

## Stage 5: Scoring

Four scorers run **sequentially** on the 863 remaining candidates.

### Scorer 1: PhoenixScorer

Sends all 863 candidates + Priya's action history to the Phoenix Prediction model:

```
Predict {
    user_id:              900100200,
    user_action_sequence: <128-action history>,
    tweet_infos:          [863 candidates with tweet_id + author_id],
}
```

The Grok-based transformer processes the input sequence:
```
[Priya's embedding] [Historyâ‚] [Historyâ‚‚] ... [Historyâ‚â‚‚â‚ˆ] [Candâ‚] [Candâ‚‚] ... [Candâ‚ˆâ‚†â‚ƒ]
```

With candidate isolation masking, each candidate attends to Priya's context but NOT to other candidates.

Returns 19 probabilities per candidate. Let's follow 5 specific posts:

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Post                        â”‚ P(fav)â”‚P(reply)â”‚P(RT) â”‚P(click)â”‚P(dwell)â”‚P(share)â”‚P(block)â”‚P(report)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A: @techguru "Rust v2.0"    â”‚ 0.42  â”‚ 0.15   â”‚ 0.18 â”‚ 0.55   â”‚ 0.72   â”‚ 0.08   â”‚ 0.001  â”‚ 0.0002
   (in-network, original)   â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
                             â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
B: @designeramy "Design ğŸ§µ" â”‚ 0.38  â”‚ 0.08   â”‚ 0.22 â”‚ 0.48   â”‚ 0.65   â”‚ 0.12   â”‚ 0.001  â”‚ 0.0001
   (in-network, original)   â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
                             â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
C: @rustacean_dev "Async.." â”‚ 0.35  â”‚ 0.12   â”‚ 0.14 â”‚ 0.60   â”‚ 0.68   â”‚ 0.06   â”‚ 0.002  â”‚ 0.0003
   (OUT-of-network)         â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
                             â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
D: @techguru "reply to..."  â”‚ 0.18  â”‚ 0.05   â”‚ 0.04 â”‚ 0.30   â”‚ 0.40   â”‚ 0.02   â”‚ 0.001  â”‚ 0.0001
   (in-network, reply)      â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
                             â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
E: @sportsfan "Game recap"  â”‚ 0.12  â”‚ 0.03   â”‚ 0.05 â”‚ 0.20   â”‚ 0.35   â”‚ 0.01   â”‚ 0.003  â”‚ 0.0005
   (in-network, original)   â”‚       â”‚        â”‚      â”‚        â”‚        â”‚        â”‚        â”‚
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Time elapsed: ~55ms** (ML inference is the most expensive step)

### Scorer 2: WeightedScorer

Combines all 19 predicted probabilities using configured weights. Let's use example weights:

```
Positive weights (simplified):
  FAVORITE_WEIGHT        = 1.0
  REPLY_WEIGHT           = 27.0
  RETWEET_WEIGHT         = 1.0
  CLICK_WEIGHT           = 0.1
  PROFILE_CLICK_WEIGHT   = 1.0
  VQV_WEIGHT             = 0.3   (only if video > MIN_VIDEO_DURATION_MS)
  SHARE_WEIGHT           = 1.0
  SHARE_VIA_DM_WEIGHT    = 1.0
  SHARE_VIA_COPY_LINK    = 1.0
  DWELL_WEIGHT           = 0.2
  QUOTE_WEIGHT           = 1.0
  FOLLOW_AUTHOR_WEIGHT   = 10.0
  CONT_DWELL_TIME_WEIGHT = 0.05

Negative weights:
  NOT_INTERESTED_WEIGHT  = -74.0
  BLOCK_AUTHOR_WEIGHT    = -74.0
  MUTE_AUTHOR_WEIGHT     = -74.0
  REPORT_WEIGHT          = -200.0
```

**Calculation for Post A** (@techguru "Rust v2.0", no video):
```
weighted = (0.42 Ã— 1.0)    fav
         + (0.15 Ã— 27.0)   reply      = 4.05
         + (0.18 Ã— 1.0)    RT
         + (0.55 Ã— 0.1)    click      = 0.055
         + (0.72 Ã— 0.2)    dwell      = 0.144
         + (0.08 Ã— 1.0)    share
         + (0.001 Ã— -74.0) block      = -0.074
         + (0.0002 Ã— -200) report     = -0.04
         + ... other terms ...
         â‰ˆ 5.62

After offset_score and normalize_score â†’ weighted_score â‰ˆ 5.85
```

**Calculation for Post E** (@sportsfan "Game recap"):
```
weighted = (0.12 Ã— 1.0)    fav
         + (0.03 Ã— 27.0)   reply      = 0.81
         + (0.05 Ã— 1.0)    RT
         + (0.20 Ã— 0.1)    click      = 0.02
         + (0.35 Ã— 0.2)    dwell      = 0.07
         + (0.01 Ã— 1.0)    share
         + (0.003 Ã— -74.0) block      = -0.222
         + (0.0005 Ã— -200) report     = -0.10
         + ... other terms ...
         â‰ˆ 1.15

After offset_score and normalize_score â†’ weighted_score â‰ˆ 1.28
```

Notice how the **high reply weight (27Ã—)** dramatically boosts posts that Priya is likely to reply to. The negative weights on block/report **push down** sketchy content.

**Results after WeightedScorer** (our 5 tracked posts):
```
Post A: @techguru "Rust v2.0"        â†’ weighted_score: 5.85
Post B: @designeramy "Design ğŸ§µ"     â†’ weighted_score: 5.12
Post C: @rustacean_dev "Async..."     â†’ weighted_score: 4.88
Post D: @techguru "reply to..."       â†’ weighted_score: 2.01
Post E: @sportsfan "Game recap"       â†’ weighted_score: 1.28
```

### Scorer 3: AuthorDiversityScorer

Processes candidates in **descending weighted_score order**. Tracks how many times each author has appeared.

Assume `decay_factor = 0.5` and `floor = 0.1`:

```
multiplier(n) = (1 - 0.1) Ã— 0.5^n + 0.1

  n=0 (1st post):  0.9 Ã— 1.0  + 0.1 = 1.00   (no penalty)
  n=1 (2nd post):  0.9 Ã— 0.5  + 0.1 = 0.55   (45% penalty)
  n=2 (3rd post):  0.9 Ã— 0.25 + 0.1 = 0.325  (67% penalty)
```

Processing order (by weighted_score descending):
```
1. Post A: @techguru  weighted=5.85 â†’ author_count[techguru]=0 â†’ mult=1.00 â†’ score = 5.85 Ã— 1.00 = 5.85
2. Post B: @designeramy weighted=5.12 â†’ author_count[designeramy]=0 â†’ mult=1.00 â†’ score = 5.12 Ã— 1.00 = 5.12
3. Post C: @rustacean_dev weighted=4.88 â†’ author_count[rustacean_dev]=0 â†’ mult=1.00 â†’ score = 4.88 Ã— 1.00 = 4.88
4. Post D: @techguru  weighted=2.01 â†’ author_count[techguru]=1 â†’ mult=0.55 â†’ score = 2.01 Ã— 0.55 = 1.11 â† PENALIZED!
5. Post E: @sportsfan weighted=1.28 â†’ author_count[sportsfan]=0 â†’ mult=1.00 â†’ score = 1.28 Ã— 1.00 = 1.28
```

**Post D dropped below Post E** because it was @techguru's 2nd post. This ensures Priya's feed isn't dominated by one prolific author.

### Scorer 4: OONScorer

Adjusts out-of-network posts. Assume `OON_WEIGHT_FACTOR = 0.85`:

```
Post A: in_network=true  â†’ score unchanged: 5.85
Post B: in_network=true  â†’ score unchanged: 5.12
Post C: in_network=false â†’ score Ã— 0.85:   4.88 Ã— 0.85 = 4.15 â† reduced
Post D: in_network=true  â†’ score unchanged: 1.11
Post E: in_network=true  â†’ score unchanged: 1.28
```

**Final scores after all 4 scorers:**
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Rank â”‚ Post                          â”‚ Score â”‚ Type
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  #1   â”‚ A: @techguru "Rust v2.0"      â”‚ 5.85  â”‚ In-Network
  #2   â”‚ B: @designeramy "Design ğŸ§µ"   â”‚ 5.12  â”‚ In-Network
  #3   â”‚ C: @rustacean_dev "Async..."   â”‚ 4.15  â”‚ Out-of-Network
  ...  â”‚ ...198 more candidates...      â”‚ ...   â”‚ ...
  #202 â”‚ E: @sportsfan "Game recap"     â”‚ 1.28  â”‚ In-Network
  #203 â”‚ D: @techguru "reply to..."     â”‚ 1.11  â”‚ In-Network (penalized)
  ...  â”‚ ...660 more candidates...      â”‚ ...   â”‚ ...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**Time elapsed for all scoring: ~58ms**

---

## Stage 6: Selection

`TopKScoreSelector` sorts all 863 candidates by final `score` descending and picks top K (e.g., K=150):

```
Selected: Top 150 candidates
Discarded: 713 lower-scored candidates
```

Posts A, B, C make the cut. Post E barely makes it at position ~130. Post D doesn't make it.

**Time elapsed: <1ms** (just a sort)

---

## Stage 7: Post-Selection Processing

### VFCandidateHydrator
Calls Visibility Filtering service on the 150 selected posts:
```
tweet_id: 1893200088 â†’ SafetyResult { action: Drop("spam") }
tweet_id: 1893100055 â†’ SafetyResult { action: Drop("violence") }
All others â†’ None (safe to serve)
```

### VFFilter
Removes posts flagged by VF:
```
Removed: 2 (1 spam, 1 violent content)
Remaining: 148
```

### DedupConversationFilter  
If multiple posts are from the same conversation thread, keep only the most relevant:
```
Removed: 3 (redundant branches of same conversation)
Remaining: 145
```

**Time elapsed: ~20ms** (VF service call dominates)

---

## Stage 8: Side Effects (async)

`CacheRequestInfoSideEffect` fires in the background (doesn't block the response):
```
Cache to Strato: {
    user_id: 900100200,
    served_post_ids: [1893100001, 1893100005, 1893200001, ...142 more],
    timestamp: 1740400500000,
    request_id: "abc123-900100200"
}
```

This ensures the **next request** won't re-serve these same 145 posts.

---

## Final Response

The gRPC response goes back to Priya's X app:

```protobuf
ScoredPostsResponse {
    scored_posts: [
        {
            tweet_id:      1893100001,          // Post A
            author_id:     100200300,
            score:         5.85,
            in_network:    true,
            served_type:   ForYouInNetwork,
            screen_names:  { 100200300: "techguru" },
        },
        {
            tweet_id:      1893100005,          // Post B
            author_id:     300400500,
            score:         5.12,
            in_network:    true,
            served_type:   ForYouInNetwork,
            screen_names:  { 300400500: "designeramy" },
        },
        {
            tweet_id:      1893200001,          // Post C
            author_id:     600700800,
            score:         4.15,
            in_network:    false,
            served_type:   ForYouPhoenixRetrieval,
            screen_names:  { 600700800: "rustacean_dev" },
        },
        // ... 142 more posts in descending score order ...
    ]
}
```

**Total latency: ~130ms** (from request to response)

---

## What Priya Sees

When Priya's app renders the feed:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  For You                                          âœ¨    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  ğŸ”µ @techguru                                    Â· 2h  â”‚
â”‚  Just shipped v2.0 of our Rust framework! ğŸ¦€            â”‚
â”‚  Major perf improvements and a new async runtime...     â”‚
â”‚  â™¡ 1.2K   ğŸ’¬ 89   ğŸ” 234                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  ğŸ”µ @designeramy                                 Â· 30m â”‚
â”‚  New design system drop â€” check the thread ğŸ§µ           â”‚
â”‚  We rebuilt our entire component library from scratch... â”‚
â”‚  â™¡ 3.4K   ğŸ’¬ 156  ğŸ” 892                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  âœ¨ @rustacean_dev                                Â· 5h  â”‚
â”‚  Here's why async Rust changed our production infra...  â”‚
â”‚  [Long thread about async patterns]                     â”‚
â”‚  â™¡ 8.1K   ğŸ’¬ 342  ğŸ” 1.5K                              â”‚
â”‚  â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€  â”‚
â”‚  â†‘ Priya doesn't follow @rustacean_dev                  â”‚
â”‚    Phoenix Retrieval found this based on her history     â”‚
â”‚    of engaging with Rust + async content                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  ğŸ”µ @newsbot                                     Â· 1h  â”‚
â”‚  Breaking: New open-source ML framework released...     â”‚
â”‚  â™¡ 567    ğŸ’¬ 23   ğŸ” 189                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                         â”‚
â”‚  ... (141 more posts as she scrolls) ...                â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Notice:
- **@techguru** appears once at #1, even though they had 2 posts â€” the diversity scorer pushed the reply down
- **@rustacean_dev** (out-of-network) ranked #3 â€” Phoenix found it because Priya's history is full of Rust/async engagement
- **@annoying_person** (muted) is nowhere to be seen
- **"Crypto giveaway"** spam posts are filtered out
- Posts she already saw in her last session don't reappear

---

## Latency Breakdown

```
Stage                          â”‚ Time    â”‚ Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Query Hydration             â”‚  12ms   â”‚ UAS + Strato in parallel
2. Candidate Sources           â”‚  45ms   â”‚ Thunder (~3ms) + Phoenix Retrieval (~42ms)
3. Hydration                   â”‚  65ms   â”‚ TES + Gizmoduck + others in parallel
4. Pre-scoring Filters         â”‚   2ms   â”‚ All in-memory, sequential
5. Scoring                     â”‚  58ms   â”‚ Phoenix ML (~55ms) + weighted/diversity (~3ms)
6. Selection                   â”‚  <1ms   â”‚ Sort + truncate
7. Post-selection              â”‚  20ms   â”‚ VF service call + filters
8. Side Effects                â”‚   0ms   â”‚ Async, doesn't block response
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL (with parallelism)       â”‚ ~130ms  â”‚ Stages 1-3 overlap; 4-7 sequential
```

Stages 1, 2, and 3 overlap with each other (they're sequential stages but internally parallel), but stages 4 through 7 must execute sequentially since each depends on the previous stage's output.

---

## Why Post A Beat Post C

Let's compare @techguru's original post (in-network) vs @rustacean_dev's post (out-of-network):

```
                            Post A              Post C
                            @techguru            @rustacean_dev
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
P(favorite)                 0.42                0.35
P(reply)                    0.15                0.12
P(retweet)                  0.18                0.14
P(click)                    0.55                0.60      â† C wins here
P(dwell)                    0.72                0.68

After WeightedScorer        5.85                4.88
  (replyÃ—27 helps A a lot since 0.15>0.12)

After AuthorDiversity       5.85 (Ã—1.00)        4.88 (Ã—1.00)
  (both are 1st post from their author)

After OONScorer             5.85 (in-network)   4.88 Ã— 0.85 = 4.15
  (C gets penalized for being OON)

FINAL RANK                  #1                  #3
```

The **reply score difference** (0.15 vs 0.12) gets amplified by the 27Ã— reply weight, creating a 0.81 gap. Then the **OON penalty** widens it further. Even though Post C had a higher click probability, the system values reply potential much more heavily.

---

## What If Priya Engages?

If Priya likes Post C (@rustacean_dev), that action gets appended to her engagement history. On her **next** feed request:

1. Her `user_action_sequence` now includes `Liked tweet_id:1893200001 by author:600700800`
2. Phoenix Retrieval will find **more posts similar to @rustacean_dev's content**
3. Phoenix Ranking will give **higher scores** to posts from authors/topics similar to @rustacean_dev
4. Over time, more Rust infrastructure content appears in her feed

The transformer learns entirely from these engagement signals â€” no hand-coded rules like "if user likes Rust content, show more Rust content." The model figures out the patterns from billions of engagement sequences across all users.

---

---

# Advanced Deep Dives

> The sections below expand on the example above for engineers who want to understand the system at production depth â€” tensor shapes, attention mechanics, hash collision math, failure modes, and tuning levers.

---

## Deep Dive 1: Tensor Shapes Through the Phoenix Ranking Model

Let's trace the exact tensor dimensions as Priya's request flows through the Grok transformer for scoring. We'll use the production defaults from `PhoenixModelConfig`:

```
Config:
  emb_size (D)            = 128
  key_size (K)            = 64
  num_q_heads             = 2
  num_kv_heads            = 2
  num_layers              = 2
  widening_factor         = 2
  history_seq_len (S)     = 128       (Priya's last 128 actions)
  candidate_seq_len (C)   = 32        (batch size for candidates)
  num_actions             = 19
  num_user_hashes         = 2
  num_item_hashes         = 2
  num_author_hashes       = 2
  product_surface_vocab   = 16
  fprop_dtype             = bfloat16
```

### Step 1: Embedding Assembly

```
USER EMBEDDING:
  Hash user_id (900100200) with 2 hash functions:
    hashâ‚(900100200) â†’ index 4827 in embedding_table_1  â†’ [D]
    hashâ‚‚(900100200) â†’ index 1293 in embedding_table_2  â†’ [D]
  Concatenate â†’ [2D] = [256]
  Project via block_user_reduce: Linear([256] â†’ [D]) â†’ [128]
  Reshape â†’ [B, 1, D] = [1, 1, 128]

HISTORY EMBEDDINGS (for each of 128 past actions):
  For action i:
    post_hash_embeds:    [num_item_hashes, D] = [2, 128] â†’ concat â†’ [256]
    author_hash_embeds:  [num_author_hashes, D] = [2, 128] â†’ concat â†’ [256]
    action_embed:        lookup in action_embedding_table â†’ [D] = [128]
    surface_embed:       lookup in product_surface_table â†’ [D] = [128]
    Concatenate all: [256 + 256 + 128 + 128] = [768]
    block_history_reduce: Linear([768] â†’ [D]) â†’ [128]
  Stack all 128 â†’ [B, S, D] = [1, 128, 128]

CANDIDATE EMBEDDINGS (for 32 candidates in this batch):
  For candidate j:
    post_hash_embeds:    [num_item_hashes, D] = [2, 128] â†’ concat â†’ [256]
    author_hash_embeds:  [num_author_hashes, D] = [2, 128] â†’ concat â†’ [256]
    surface_embed:       lookup â†’ [D] = [128]
    âš  NO action embedding (candidates don't have actions yet!)
    Concatenate: [256 + 256 + 128] = [640]
    block_candidate_reduce: Linear([640] â†’ [D]) â†’ [128]
  Stack all 32 â†’ [B, C, D] = [1, 32, 128]
```

### Step 2: Sequence Assembly

```
Full input sequence:
  [User(1)] [Historyâ‚..â‚â‚‚â‚ˆ] [Candâ‚..â‚ƒâ‚‚]
  Shape: [B, 1+S+C, D] = [1, 161, 128]

  candidate_start_offset = 1 + 128 = 129
```

### Step 3: Attention Mask Construction

```python
# make_recsys_attn_mask(seq_len=161, candidate_start_offset=129)
seq_len = 161

# 1) Start with standard causal mask [1, 1, 161, 161]
causal = jnp.tril(jnp.ones((1, 1, 161, 161)))

# 2) Zero out candidate-to-candidate block
#    Rows 129-160 cannot attend to columns 129-160
attn_mask = causal.at[:, :, 129:, 129:].set(0)

# 3) Re-enable self-attention for each candidate on the diagonal
#    Position 129 can attend to position 129, etc.
for i in range(129, 161):
    attn_mask = attn_mask.at[:, :, i, i].set(1)
```

Visualized (simplified 8-token version with offset=5):
```
              User  H1   H2   H3   H4   C1   C2   C3
  User     [  1     0    0    0    0    0    0    0  ]  â† user sees self
  H1       [  1     1    0    0    0    0    0    0  ]  â† causal
  H2       [  1     1    1    0    0    0    0    0  ]
  H3       [  1     1    1    1    0    0    0    0  ]
  H4       [  1     1    1    1    1    0    0    0  ]  â† last history
  C1       [  1     1    1    1    1   [1]   0    0  ]  â† sees user+history+self
  C2       [  1     1    1    1    1    0   [1]   0  ]  â† sees user+history+self
  C3       [  1     1    1    1    1    0    0   [1] ]  â† sees user+history+self
                                       â†‘ â†‘ â†‘
                                   candidate-to-candidate = 0
                                   (except self-attention on diagonal)
```

**Why this matters for production:** Because C1's score doesn't depend on C2 or C3, you can:
- Score the same tweet in different batches and get identical results 
- Cache scored candidates and mix them with fresh ones
- Parallelize scoring across multiple GPU batches without cross-batch dependencies

### Step 4: Transformer Forward Pass

```
For each of 2 decoder layers:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Input shape: [1, 161, 128]                      â”‚
  â”‚                                                 â”‚
  â”‚ 1. RMSNorm: [1, 161, 128] â†’ [1, 161, 128]      â”‚
  â”‚                                                 â”‚
  â”‚ 2. Multi-Head Attention (GQA):                  â”‚
  â”‚    Q = input @ W_q: [1, 161, 2, 64]             â”‚
  â”‚    K = input @ W_k: [1, 161, 2, 64]             â”‚
  â”‚    V = input @ W_v: [1, 161, 2, 64]             â”‚
  â”‚    Apply RoPE to Q, K                           â”‚
  â”‚    logits = Q @ K.T / âˆš64: [1, 2, 161, 161]    â”‚
  â”‚    Cap: 30.0 Ã— tanh(logits / 30.0)             â”‚
  â”‚    Mask: logits Ã— attn_mask + (-1e10)Ã—(1-mask)  â”‚
  â”‚    Softmax â†’ attn_weights: [1, 2, 161, 161]     â”‚
  â”‚    Output = attn_weights @ V: [1, 2, 161, 64]   â”‚
  â”‚    Project: [1, 161, 128]                       â”‚
  â”‚    RMSNorm â†’ residual add                       â”‚
  â”‚                                                 â”‚
  â”‚ 3. GELU-gated FFN:                              â”‚
  â”‚    ffn_size = int(2 Ã— 128) Ã— 2 // 3            â”‚
  â”‚            = 170 â†’ round up to 176 (multiple 8) â”‚
  â”‚    h_gate = GELU(input @ W_gate): [1, 161, 176] â”‚
  â”‚    h_up   = input @ W_up:         [1, 161, 176] â”‚
  â”‚    h_combined = h_gate * h_up:    [1, 161, 176] â”‚
  â”‚    output = h_combined @ W_down:  [1, 161, 128] â”‚
  â”‚    RMSNorm â†’ residual add                       â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Note:** The FFN uses `GELU` (Gaussian Error Linear Unit), not SiLU. This differs from some Grok documentation but matches the actual `grok.py` implementation.

### Step 5: Output Projection

```
transformer_output: [1, 161, 128]

# Extract only candidate positions
candidate_embeddings = layer_norm(transformer_output)[:, 129:, :]
  â†’ [1, 32, 128]

# Project to action logits
logits = candidate_embeddings @ unembedding_matrix
  â†’ [1, 32, 19]     (unembedding_matrix: [128, 19])

# Convert to probabilities
probs = sigmoid(logits)   # in Python runner
  â†’ [1, 32, 19]

# Each of the 32 candidates now has 19 probability scores
```

### Step 6: Batched Scoring for 863 Candidates

Since Priya has 863 candidates but `candidate_seq_len = 32`:

```
Total batches = ceil(863 / 32) = 27 batches
Last batch: 863 - 26Ã—32 = 31 candidates (padded to 32)

Each batch shares the same [User + History] prefix.
The transformer processes [1, 161, 128] per batch.

Total forward passes: 27
Effective throughput: 863 candidates scored in ~55ms
  â†’ ~0.064ms per candidate
  â†’ ~2ms per batch of 32
```

---

## Deep Dive 2: Hash-Based Embedding â€” Collision Analysis

The system uses **multiple hash functions** per entity to reduce collision rates while keeping embedding tables bounded.

### How It Works

For tweet ID `1893100001` with `num_item_hashes = 2`:

```
Hash function 1: hashâ‚(1893100001) mod TABLE_SIZE â†’ slot 84,291
Hash function 2: hashâ‚‚(1893100001) mod TABLE_SIZE â†’ slot 12,507

embeddingâ‚ = table_1[84291]  â†’ [D] = [128]
embeddingâ‚‚ = table_2[12507]  â†’ [D] = [128]

concatenated = [embeddingâ‚ | embeddingâ‚‚] â†’ [2D] = [256]
projected = Linear([256] â†’ [128]) â†’ [128]
```

### Collision Probability

With a single hash function into a table of size T, the probability that two distinct IDs collide:

$$P(\text{collision}) = \frac{1}{T}$$

With 2 independent hash functions, two IDs must collide in **both** tables simultaneously:

$$P(\text{both collide}) = \frac{1}{T_1} \times \frac{1}{T_2} = \frac{1}{T^2}$$

For T = 100,000:
- Single hash: 1 in 100K collision rate
- Dual hash: 1 in 10 billion collision rate

Even with partial collisions (same slot in one table, different in the other), the concatenation + learned projection allows the model to disambiguate:

```
Tweet A: [emb_aâ‚ | emb_sharedâ‚‚]  â†’ project â†’ different output
Tweet B: [emb_bâ‚ | emb_sharedâ‚‚]  â†’ project â†’ different output
                   â†‘ same slot in table 2, but table 1 differs
```

### Memory Trade-off

```
Naive approach (unique embedding per entity):
  50M tweets Ã— 128 floats Ã— 4 bytes = 25.6 GB

Hash-based approach (2 tables Ã— 100K slots):
  2 Ã— 100,000 Ã— 128 Ã— 4 bytes = 102.4 MB  â† 250Ã— smaller
  + 1 projection matrix: 256 Ã— 128 Ã— 4 = 131 KB

Total: ~103 MB vs 25.6 GB
```

The same approach is used for user IDs (2 hash tables) and author IDs (2 hash tables), keeping the entire embedding infrastructure under ~300 MB.

---

## Deep Dive 3: Two-Tower Retrieval â€” Similarity Geometry

Let's trace how Phoenix Retrieval finds Post C (@rustacean_dev's async Rust post) for Priya.

### User Tower Processing

Priya's engagement sequence goes through the full transformer â†’ mean pool â†’ L2 normalize:

```
Input: [User_emb | Historyâ‚ | ... | Historyâ‚â‚‚â‚ˆ]  â†’  [1, 129, 128]

Transformer output: [1, 129, 128]

Mean pool (over valid positions):
  user_repr = mean(output[:, :valid_len, :])  â†’  [1, 128]

L2 Normalize:
  user_repr = user_repr / ||user_repr||â‚‚  â†’  [1, 128]

  ||user_repr||â‚‚ = 1.0  (unit sphere)
```

### Candidate Tower Processing (Offline)

For every tweet in the corpus, the Candidate Tower runs a lightweight 2-layer MLP:

```
For tweet 1893200001 (@rustacean_dev's post):
  post_hashes:   concat(hashâ‚, hashâ‚‚) â†’ [256]
  author_hashes: concat(hashâ‚, hashâ‚‚) â†’ [256]
  Total input: [512]

  Layer 1: Linear(512 â†’ 256) â†’ SiLU â†’ [256]
  Layer 2: Linear(256 â†’ 128) â†’ [128]
  L2 Normalize â†’ [128]

  ||candidate_repr||â‚‚ = 1.0  (unit sphere)
```

### Dot-Product Retrieval

Since both vectors are unit-normalized, dot product = cosine similarity:

$$\text{score}(u, c) = \vec{u} \cdot \vec{c} = \|\vec{u}\| \|\vec{c}\| \cos\theta = \cos\theta$$

```
Priya's user embedding:       u = [0.12, -0.08, 0.15, 0.22, ..., 0.05]  (128-dim)

Corpus (millions of candidate embeddings):
  tweet_1893200001 (Rust async): câ‚ = [0.14, -0.06, 0.13, 0.20, ..., 0.07]
  tweet_1893200099 (crypto spam): câ‚‚ = [-0.20, 0.15, -0.08, -0.12, ..., 0.18]
  tweet_1893200050 (cooking vid):  câ‚ƒ = [0.02, 0.25, -0.15, 0.04, ..., -0.22]

  score(u, câ‚) = 0.87  â† high similarity (Rust/async aligns with Priya's history)
  score(u, câ‚‚) = 0.12  â† low similarity (crypto spam is far away)
  score(u, câ‚ƒ) = 0.23  â† low (cooking content doesn't match)
```

The `jax.lax.top_k(scores, k=500)` efficiently extracts the 500 highest-scoring candidates without sorting the full corpus.

### Why Separate Towers?

```
ONLINE (per request, ~42ms):
  Only the User Tower runs â†’ 1 embedding computed
  Dot product against pre-computed corpus â†’ ANN lookup

OFFLINE (batch job, runs continuously):
  Candidate Tower processes all new/updated tweets
  Writes embeddings to a vector index

If both towers were combined (single model):
  Would need to score every corpus tweet against every user â†’ O(Users Ã— Tweets) per request
  Completely infeasible at scale
```

---

## Deep Dive 4: Visibility Filtering â€” Dual Safety Levels

After selection, the `VFCandidateHydrator` applies different safety thresholds depending on whether a post is in-network or out-of-network:

```
In-network posts:   SafetyLevel::TimelineHome
  â†’ Lower threshold â€” user explicitly chose to follow this author
  â†’ Only blocks: deleted, suspended accounts, hard spam, CSAM

Out-of-network posts: SafetyLevel::TimelineHomeRecommendations
  â†’ Higher threshold â€” system is recommending this content
  â†’ Also blocks: borderline content, low-quality, sensational,
    unverified claims, graphic media without labels
```

Both safety levels are evaluated **in parallel** via `futures::future::join`:

```rust
let (in_network_results, oon_results) = futures::future::join(
    vf_client.check(in_network_ids, TimelineHome),
    vf_client.check(oon_ids, TimelineHomeRecommendations),
).await;
```

### In Our Example

```
Selected 150 posts â†’ split by in_network:
  In-network (89 posts)  â†’ VF with TimelineHome
  OON        (61 posts)  â†’ VF with TimelineHomeRecommendations

Results:
  In-net: 0 removed (all safe for TimelineHome)
  OON:    2 removed
    - tweet_1893200088: spam (caught even at lower threshold)
    - tweet_1893100055: graphic violence without labels (caught at stricter OON threshold
      â€” would have passed TimelineHome threshold if from a followed account)
```

This asymmetry means Priya's feed from followed accounts is more permissive (she opted into that content), while recommended content goes through stricter quality gates.

---

## Deep Dive 5: The `PreviouslyServedPostsFilter` Enable Gate

A subtle but important detail: this filter has a conditional `enable()` override:

```rust
fn enable(&self, query: &ScoredPostsQuery) -> bool {
    query.is_bottom_request
}
```

### What This Means

```
Pull-to-refresh (top of feed):
  is_bottom_request = false
  â†’ PreviouslyServedPostsFilter is DISABLED
  â†’ Previously served posts CAN reappear
  â†’ Rationale: user might want to re-read posts from their last session

Scroll-to-bottom (infinite scroll):
  is_bottom_request = true
  â†’ PreviouslyServedPostsFilter is ENABLED
  â†’ Previously served posts are REMOVED
  â†’ Rationale: showing the same post twice while scrolling feels broken
```

In Priya's case, she opened the app fresh (`is_bottom_request: false`), so this filter was **skipped**. If she had been scrolling to load more, it would have removed 8 additional posts.

---

## Deep Dive 6: Weight Sensitivity Analysis

The `WeightedScorer` weights dramatically shape feed character. Let's see how different weight configurations would change Priya's feed:

### Current Configuration (Reply-Heavy)

```
REPLY_WEIGHT = 27.0  (dominant signal)
```

```
Post A: @techguru "Rust v2.0"      â†’ weighted = 5.85  (#1)
Post C: @rustacean_dev "Async..."   â†’ weighted = 4.88  (#3 before OON penalty)
```

The high reply weight means **conversational content** dominates â€” posts Priya would likely respond to rank highest.

### Hypothetical: Share-Heavy Configuration

```
SHARE_WEIGHT = 27.0 (swap with reply)
REPLY_WEIGHT = 1.0
```

```
Post A recalculated:
  reply term:  0.15 Ã— 1.0  = 0.15  (was 4.05)
  share term:  0.08 Ã— 27.0 = 2.16  (was 0.08)
  â†’ The Post A advantage shrinks; @designeramy's share_score (0.12 Ã— 27 = 3.24) leaps ahead.

New ranking would likely be:
  #1: Post B (@designeramy) â€” high shareability
  #2: Post A (@techguru)    â€” still strong but less dominant
  #3: Post C (@rustacean_dev)
```

### Hypothetical: Safety-Aggressive Configuration

```
BLOCK_AUTHOR_WEIGHT = -200.0  (up from -74.0)
REPORT_WEIGHT       = -500.0  (up from -200.0)
```

```
Posts from authors with even slightly elevated block/report probabilities
get pushed far down. The feed becomes extremely "safe" but may feel bland â€”
controversial but legitimate discussion gets suppressed.

Post E recalculated:
  block term:  0.003 Ã— -200.0 = -0.60   (was -0.222)
  report term: 0.0005 Ã— -500.0 = -0.25  (was -0.10)
  â†’ Post E's score drops to ~0.63 (from 1.28) â€” it likely falls off the top 150
```

### The `offset_score` Mechanism

The `WeightedScorer` has a subtle normalization for negative combined scores:

```
If combined < 0.0:
  offset = (combined + |NEGATIVE_WEIGHTS_SUM|) / WEIGHTS_SUM Ã— NEGATIVE_SCORES_OFFSET

This ensures strongly-negative posts don't get arbitrarily negative scores.
Instead, they're compressed into a small negative range â€” preventing a single
bad probability from completely dominating the ranking.
```

---

## Deep Dive 7: Author Diversity â€” Exponential Decay Walkthrough

Let's trace what happens when a prolific author like @techguru has 5 posts surviving the filter stage.

```
decay_factor = 0.5, floor = 0.1
multiplier(n) = (1.0 - 0.1) Ã— 0.5^n + 0.1

n=0: 0.9 Ã— 1.000 + 0.1 = 1.000   (1st post, no penalty)
n=1: 0.9 Ã— 0.500 + 0.1 = 0.550   (45% cut)
n=2: 0.9 Ã— 0.250 + 0.1 = 0.325   (67.5% cut)
n=3: 0.9 Ã— 0.125 + 0.1 = 0.213   (78.8% cut)
n=4: 0.9 Ã— 0.0625+ 0.1 = 0.156   (84.4% cut)

As nâ†’âˆ: multiplier â†’ floor = 0.1  (maximum 90% penalty, never fully zero)
```

### Scoring Order Matters

Candidates are processed in **descending `weighted_score` order**. This means the BEST post from each author always gets `multiplier = 1.0`:

```
Processing order across ALL candidates (863 total):
  #1: @guru_rust    weighted=8.22 â†’ first by @guru_rust      â†’ Ã—1.000 â†’ score=8.22
  #2: @techguru     weighted=5.85 â†’ first by @techguru (Post A) â†’ Ã—1.000 â†’ score=5.85
  #3: @designeramy  weighted=5.12 â†’ first by @designeramy     â†’ Ã—1.000 â†’ score=5.12
  ...
  #87: @techguru    weighted=2.01 â†’ SECOND by @techguru (Post D) â†’ Ã—0.550 â†’ score=1.11
  ...
  #201: @techguru   weighted=0.82 â†’ THIRD by @techguru         â†’ Ã—0.325 â†’ score=0.27
  ...
```

If we sorted AFTER diversity scoring (which we do in the selector), the ranking naturally interleaves authors without any explicit interleaving algorithm.

### Why floor > 0?

Without a floor (`floor = 0.0`), the 10th post from an author would score `0.5^9 Ã— original â‰ˆ 0.2%` of its value â€” effectively zero. With `floor = 0.1`, even the 100th post retains 10% of its score. This means truly exceptional content from a prolific author can still rank well.

---

## Deep Dive 8: Thunder's Real-Time Ingestion Path

### From Tweet Creation to PostStore

```
Timeline:
  T+0ms:    User posts a tweet
  T+~5ms:   Tweet create event written to Kafka topic
  T+~15ms:  Thunder's Kafka consumer (v2) picks up the protobuf InNetworkEvent
  T+~16ms:  Deserialize â†’ LightPost
  T+~17ms:  Insert into PostStore (DashMap lock-free write)
  T+~17ms:  Tweet is now available for in-network queries
```

### PostStore Insert Logic

```
For each new post (e.g., @techguru's "Rust v2.0"):
  1. Validate:
     - NOT in deleted_posts set?           âœ“
     - created_at < now + small_buffer?     âœ“ (no future posts)
     - created_at > now - retention_secs?   âœ“ (not too old)

  2. Insert into main store:
     posts.insert(1893100001, LightPost { post_id: 1893100001, author_id: 100200300, ... })

  3. Classify and index:
     is_reply?    â†’ No
     is_retweet?  â†’ No
     â†’ original_posts_by_user[100200300].push_front(TinyPost { post_id: 1893100001, ... })

     has_video?   â†’ No
     â†’ (skip video index)

  4. Enforce per-author caps:
     if original_posts_by_user[100200300].len() > MAX_ORIGINAL_POSTS_PER_AUTHOR:
         remove oldest from deque (and from main posts map)
```

### Memory Model

```
LightPost (per post, ~120 bytes):
  post_id: i64, author_id: i64, created_at: i64,
  in_reply_to_post_id: Option<i64>, in_reply_to_user_id: Option<i64>,
  is_retweet: bool, is_reply: bool,
  source_post_id: Option<i64>, source_user_id: Option<i64>,
  has_video: bool, conversation_id: Option<i64>

TinyPost (per timeline index entry, ~16 bytes):
  post_id: i64, created_at: i64

For 10M active posts:
  Main store:     10M Ã— 120B â‰ˆ 1.2 GB
  Timeline indexes: ~30M entries Ã— 16B â‰ˆ 480 MB  (3 indexes: original, secondary, video)
  Overhead:       ~500 MB (DashMap overhead, deleted tracking, etc.)
  Total:          ~2.2 GB

This fits comfortably in a single machine's memory, enabling sub-millisecond lookups.
```

### Auto-Trim Cycle

```
Every 2 minutes:
  1. Scan all posts in DashMap
  2. For each post where (now - created_at) > retention_seconds:
     - Remove from posts map
     - Remove TinyPost from the appropriate per-user deque
     - Add to deleted_posts set (prevents re-insertion from delayed Kafka events)
  3. Log: "Trimmed 42,891 expired posts, 12,431,607 remaining"
```

---

## Deep Dive 9: Failure Modes & Resilience

### What Happens When Phoenix ML Is Down?

```
PhoenixSource.get_candidates() â†’ Error("gRPC timeout after 5000ms")
PhoenixScorer.score() â†’ Error("gRPC timeout after 5000ms")

Pipeline behavior (from candidate_pipeline.rs):
  - Source error: logged, source returns empty vec, pipeline continues
  - Scorer error: logged, scorer is skipped, pipeline continues

Result for Priya:
  - Only Thunder (in-network) candidates survive
  - No PhoenixScores â†’ WeightedScorer assigns weighted_score = 0.0 for all
  - AuthorDiversityScorer still works (uses weighted_score)
  - Feed falls back to roughly chronological in-network content
  - User experience: "For You" looks like "Following" tab temporarily
```

### What Happens When TES Is Slow?

```
CoreDataCandidateHydrator takes 500ms instead of 30ms

Impact:
  - Total latency jumps from ~130ms to ~550ms
  - User sees a longer loading spinner
  - But all data is correct â€” just slow

Mitigation:
  - PostStore.request_timeout: if hydration exceeds this, remaining candidates
    get default/empty values and are caught by CoreDataHydrationFilter
  - Partial degradation: 800 of 987 candidates hydrate successfully,
    187 get filtered out, feed is slightly less diverse but still good
```

### What Happens When a Kafka Partition Lags?

```
Thunder Kafka consumer v2 falls behind by 5 minutes on partition 7

Impact:
  - Posts from authors whose events land on partition 7 are delayed
  - Priya might not see @techguru's latest post for 5 minutes
  - Thunder's partition lag monitoring surfaces this in metrics
  - Other partitions are unaffected

Mitigation:
  - Multiple partitions ensure most authors' posts arrive on time
  - Phoenix Retrieval (not dependent on Thunder) can still surface those posts
    if they're in the cached corpus embeddings
```

### Hydrator Length Mismatch Safety

```
The pipeline framework has a critical safety check:

If hydrator returns N results but pipeline has M candidates (N â‰  M):
  â†’ Warning logged: "Hydrator returned {N} results, expected {M}"
  â†’ Hydrator results are DISCARDED entirely
  â†’ Pipeline continues with un-hydrated candidates
  â†’ Not a crash â€” graceful degradation

This prevents data misalignment bugs where candidate[i] gets candidate[j]'s data.
```

---

## Deep Dive 10: End-to-End Latency Optimization

### Parallelism Map

```
                    0ms    20ms   40ms   60ms   80ms  100ms  120ms  130ms
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Query Hydration:    â”œâ”€â”€UASâ”€â”€â”¤
                    â”œâ”€â”€Stratoâ”€â”¤
                    â”‚      â”‚      â”‚
Candidate Sources:  â”‚      â”œâ”€â”€Thunderâ”€â”€â”¤
                    â”‚      â”œâ”€â”€â”€â”€Phoenix Retrievalâ”€â”€â”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚    â”‚
Candidate Hydration:â”‚      â”‚      â”‚      â”‚      â”œâ”€InNetâ”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”œâ”€TESâ”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”œâ”€Videoâ”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”œâ”€Subâ”€â”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”œâ”€Gizmoâ”€â”€â”€â”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚
Pre-scoring Filters:â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”œâ”€â”¤
Scoring:            â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”œâ”€â”€PhoenixMLâ”€â”€â”€â”€â”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”œâ”€Weightedâ”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”œâ”€Authorâ”€â”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”œOONâ”¤â”‚
Selection:          â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”‚  â”œâ”¤â”‚
Post-Selection:     â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”‚  â”‚â”œVFâ”€â”¤
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”‚  â”‚â”‚ â”œâ”¤â”‚
Side Effects:       â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”‚  â”‚â”‚ â”‚â”œâ†’ (async, non-blocking)
                    â”‚      â”‚      â”‚      â”‚      â”‚      â”‚    â”‚ â”‚       â”‚ â”‚â”‚  â”‚â”‚ â”‚
Response sent â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”¼â”¼â”€â”€â”¼â”¼â”€â”¤
                    0ms    20ms   40ms   60ms   80ms  100ms  120ms  130ms
```

### Where Time Is Spent

```
Breakdown by category:

  ML Inference:     55ms  (42%)  â€” Phoenix Retrieval + Phoenix Ranking
  Network calls:    45ms  (35%)  â€” TES, Gizmoduck, Strato, VF, UAS
  In-memory ops:     5ms   (4%)  â€” Filters, diversity, selection, OON
  Framework:        25ms  (19%)  â€” gRPC overhead, serialization, async scheduling

ML dominates. To reduce latency, you'd optimize:
  1. Reduce candidate_seq_len (fewer candidates per batch â†’ fewer batches)
  2. Use smaller transformer (fewer layers/heads)
  3. Pre-score popular content (use candidate isolation for caching)
  4. Quantize model to int8 (reduce inference time by ~2Ã—)
```

### Side Effect Timing

```
CacheRequestInfoSideEffect:
  enable() check: APP_ENV == "prod" && !in_network_only
  If enabled: tokio::spawn(async { strato_client.store_request_info(...) })
  â†’ Returns immediately, Strato write happens in background (~25ms)
  â†’ If Strato write fails, it's logged but doesn't affect the user
  â†’ Next request might re-serve some posts (acceptable degradation)
```

---

## Deep Dive 11: The Full PhoenixScores Object

Here are all 19 discrete action scores + 1 continuous action, with what they measure and how they flow through the system:

```
PhoenixScores {
  // === Positive engagement signals (positive weights in WeightedScorer) ===
  favorite_score:            Option<f64>,  // P(user likes/hearts the post)
  reply_score:               Option<f64>,  // P(user replies) â€” heaviest weight (27Ã—)
  retweet_score:             Option<f64>,  // P(user reposts) â€” called "repost_score" in Python
  photo_expand_score:        Option<f64>,  // P(user taps to expand a photo)
  click_score:               Option<f64>,  // P(user clicks the post to read thread)
  profile_click_score:       Option<f64>,  // P(user clicks author's profile)
  vqv_score:                 Option<f64>,  // P(video quality view) â€” only weighted if
                                           //   video_duration_ms > MIN_VIDEO_DURATION_MS
  share_score:               Option<f64>,  // P(user shares via any channel)
  share_via_dm_score:        Option<f64>,  // P(user shares via direct message)
  share_via_copy_link_score: Option<f64>,  // P(user copies link)
  dwell_score:               Option<f64>,  // P(user dwells/pauses on post)
  quote_score:               Option<f64>,  // P(user quote-tweets)
  quoted_click_score:        Option<f64>,  // P(user clicks through to quoted content)
  follow_author_score:       Option<f64>,  // P(user follows the author) â€” weight 10Ã—

  // === Negative signals (negative weights push score DOWN) ===
  not_interested_score:      Option<f64>,  // P(user marks "not interested") â€” weight -74Ã—
  block_author_score:        Option<f64>,  // P(user blocks the author) â€” weight -74Ã—
  mute_author_score:         Option<f64>,  // P(user mutes the author) â€” weight -74Ã—
  report_score:              Option<f64>,  // P(user reports the post) â€” weight -200Ã—

  // === Continuous signals ===
  dwell_time:                Option<f64>,  // Predicted dwell time in ms â€” weight 0.05Ã—
}
```

The `Option<f64>` type means each score can be `None` (if Phoenix ML was unavailable or the candidate wasn't scored). The `WeightedScorer` treats `None` as `0.0`:

```rust
fn apply(score: Option<f64>, weight: f64) -> f64 {
    score.unwrap_or(0.0) * weight
}
```

---

**Navigation:** [Home](index.md) Â· [Architecture](ARCHITECTURE.md) Â· **Example Scenario** Â· [Pipeline Diagram](diagrams/pipeline-component-flow.md) Â· [Sequence Diagram](diagrams/sequence-diagram.md)
